{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndamtruong2k/A-to-Z-Resources-for-Students/blob/master/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2K04DL6C8OR"
      },
      "source": [
        "# Generative Chatbot\n",
        "\n",
        "- Prepared by Zhu Lin Ch'ng, Avinash Fernando, Matthew Stevenson and Thomas Whiteley   for the University of Liverpool CSCK507 Natural Language Processing and Understanding June 2023 group project.\n",
        "- This notebook implements a seq-2-seq recurrent neural network architecture comparing the performance with and without the use of Luong Attention.\n",
        "- The detailed training logs can be accessed at https://tensorboard.dev/experiment/5hXpB3jpRWqqWu3HnrfXGw/\n",
        "- The model weights and demonstration can be accessed at https://huggingface.co/spaces/csck507/Seq2Seq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ-B8p0kC8OS"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Importing Libraries](#Importing-Libraries)\n",
        "- [Data Preprocessing](#Data-Preprocessing)\n",
        "    - [Importing the dataset](#Importing-the-dataset)\n",
        "    - [Preparing the dataset](#Preparing-the-dataset)\n",
        "        - [Read Data](#Read-Data)\n",
        "        - [Preprocess Data and structure it](#Preprocess-Data-and-structure-it)\n",
        "        - [Load the structured data into dataframe and index it](#Load-the-structured-data-into-dataframe-and-index-it)\n",
        "        - [Create tensors](#Create-tensors)\n",
        "        - [Split and batch the data](#Split-and-batch-the-data)\n",
        "- [Model Development](#Model-Development)\n",
        "    - [Building the seq2seq model](#Building-the-seq2seq-model)\n",
        "    - [Training the seq2seq model](#Training-the-seq2seq-model)\n",
        "- [Model Evaluation](#Model-Evaluation)\n",
        "    - [Using the seq2seq models](#Using-the-seq2seq-models)\n",
        "    - [Evaluating the seq2seq models](#Evaluating-the-seq2seq-models)\n",
        "        - [BLEU Score](#BLEU-Score)\n",
        "        - [Cosine Similarity](#Cosine-Similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46iWqIM2C8OU"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "This notebook uses the following 3rd party libraries:\n",
        "- [pytorch](https://pytorch.org/): machine learning library\n",
        "- [pandas](https://pandas.pydata.org): data analysis library\n",
        "- [numpy](https://numpy.org): mathematical function library\n",
        "- [spacy](https://spacy.io/): natural language processing library\n",
        "- [nltk](https://www.nltk.org/): natural language processing library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6GojJpYuC8OU",
        "outputId": "40167990-4e57-40cf-aba4-1e7ca34fa595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import tarfile\n",
        "import unicodedata\n",
        "import zipfile\n",
        "from io import open\n",
        "from typing import Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import optim\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F-X05JZRC8OV",
        "outputId": "5a74402d-4ff9-4748-9c8e-c5b4f02c4dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Initialise the torch device, favouring GPU if available\n",
        "spacy.prefer_gpu()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "# load english dictionary for spacy\n",
        "try:\n",
        "    spacy.load('en_core_web_sm')\n",
        "except LookupError:\n",
        "    print('Run: python -m spacy download en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S22jxLW9C8OW"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "The [Ubuntu dialogue corpus](https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus) is selected to provide conversational data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOupfj-SC8OW"
      },
      "source": [
        "### Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EqQYYxJLC8OW"
      },
      "outputs": [],
      "source": [
        "def download_file(url, dir):\n",
        "    \"\"\"\n",
        "    Download file from url\n",
        "    :param url: url of file\n",
        "    :param filename: name of file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    r = requests.get(url)\n",
        "    if url.endswith('.tar.gz'):\n",
        "        z = tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\")\n",
        "        z.extractall(dir)\n",
        "        z.close()\n",
        "    elif url.endswith('.zip'):\n",
        "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "        z.extractall(dir)\n",
        "    else:\n",
        "        print('Unknown file type')\n",
        "    return None\n",
        "\n",
        "def extract_zip(filename, dir):\n",
        "    \"\"\"\n",
        "    Extract zip file\n",
        "    :param filename: name of file\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    z = zipfile.ZipFile(filename)\n",
        "    z.extractall(dir)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_RZaJf-XC8OX"
      },
      "outputs": [],
      "source": [
        "datasets = {'ubuntu-dialogue': 'data/ubuntu dialogue.zip'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D7yhbsPIC8OY",
        "outputId": "e2bb6435-e0e9-4fc5-c5d6-1a49f1c57b2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually download ubuntu dialogue dataset from https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus/download?datasetVersionNumber=2 and place in data folder\n"
          ]
        }
      ],
      "source": [
        "# Create directory\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Check if data is already downloaded\n",
        "for dataset, source in datasets.items():\n",
        "    if os.path.exists('data/' + dataset):\n",
        "        print(dataset + ' already exists')\n",
        "    elif dataset == 'ubuntu-dialogue':\n",
        "        ubuntu = 'data/ubuntu dialogue'\n",
        "        if os.path.exists(source):\n",
        "            os.makedirs(ubuntu)\n",
        "            extract_zip(source, ubuntu)\n",
        "            os.remove(source)\n",
        "            print(dataset + ' extracted')\n",
        "        elif os.path.exists(ubuntu):\n",
        "            print(dataset + ' already exists')\n",
        "        else:\n",
        "            kag = 'https://www.kaggle.com/datasets/rtatman/ubuntu-dialogue-corpus/download?datasetVersionNumber=2'\n",
        "            print(f'Manually download ubuntu dialogue dataset from {kag} and place in data folder')\n",
        "    else:\n",
        "        download_file(source, 'data')\n",
        "        print(dataset + ' downloaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2IIyqZBC8OY"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "\n",
        "The Ubuntu dataset is presented as dialogues with 3 entries with varying formats of questions (Q) and answers (A) including:\n",
        "- QQQ\n",
        "- AAA\n",
        "- QQA\n",
        "- QAA\n",
        "- QAQ\n",
        "\n",
        "These formats are parsed to produce quesion answer pairs as follows:\n",
        "- QQQ -> ignored\n",
        "- AAA -> ignored\n",
        "- QQA -> (Q+Q) - A\n",
        "- QAA -> Q - (A+A)\n",
        "- QAQ -> Q - A, A - Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YuT69MBC8OY"
      },
      "source": [
        "### Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ga8pzX0_C8Oa",
        "outputId": "66686701-0bd3-4a55-a719-69ce0bb3003b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/dialogueText.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a20a429bc60c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             'large':'_301'}\n\u001b[1;32m      4\u001b[0m \u001b[0mubuntufile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'data/dialogueText{variants[\"small\"]}.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mubuntufile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dialogueID'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dialogueID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dialogueText.csv'"
          ]
        }
      ],
      "source": [
        "variants = {'small':'',\n",
        "            'medium':'_196',\n",
        "            'large':'_301'}\n",
        "ubuntufile = f'sample_data/dialogueText{variants[\"small\"]}.csv'\n",
        "text_df = pd.read_csv(ubuntufile, quoting=3, on_bad_lines='warn')\n",
        "text_df['dialogueID'] = pd.to_numeric(text_df['dialogueID'], errors='coerce').fillna(0).astype(int)\n",
        "print(text_df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aNV90o-C8Oa"
      },
      "outputs": [],
      "source": [
        "# preview text from ubuntu dialogue dataset\n",
        "text_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQiJ1afvC8Oa"
      },
      "outputs": [],
      "source": [
        "# reduce df size\n",
        "# text_df = text_df[:50000]\n",
        "# text_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GetoGNieC8Ob"
      },
      "source": [
        "### Preprocess Data and structure it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LO-AAf5C8Ob"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm') # load spacy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f63MOX21C8Ob"
      },
      "outputs": [],
      "source": [
        "def parse_dialogue(data):\n",
        "    dialogues = {}\n",
        "    df = data.copy()\n",
        "    df.reset_index(inplace=True)\n",
        "    # Group by dialogueID\n",
        "    for dialogue_id, group in df.groupby('dialogueID'):\n",
        "        sentence_pairs = {}\n",
        "        context = ''\n",
        "        previous_direction = (None, None)\n",
        "        # Use enumerate to get the index within the group\n",
        "        for i, (_, row) in enumerate(group.iterrows()):\n",
        "            idx = i  # Use the group index instead of the original index\n",
        "            sender = row['from']\n",
        "            recipient = row['to']\n",
        "            response = str(row['text'])\n",
        "            direction = (sender, recipient)\n",
        "\n",
        "            if direction == previous_direction:\n",
        "                # add to the response to the previous message if the current message is consecutive\n",
        "                prev_idx = idx - 1\n",
        "                while prev_idx not in sentence_pairs and prev_idx >= 0: #prev_idx >=0 added in case prev_idx is -1\n",
        "                    prev_idx -= 1\n",
        "\n",
        "                if prev_idx in sentence_pairs:\n",
        "                    response = context + ' ' + response\n",
        "                    sentence_pairs[prev_idx] = (sentence_pairs[prev_idx][0], response)\n",
        "            elif (direction == previous_direction[::-1]) or (previous_direction[1] is None and direction[1] == previous_direction[0]):\n",
        "                # if the current message is from the previous recipient to the previous sender\n",
        "                # if the previous message did not have a recipient, but the current message is to the previous sender\n",
        "                sentence_pairs[idx]=(context, response)\n",
        "            else:\n",
        "                sentence_pairs[idx]=(context, response)\n",
        "\n",
        "            previous_direction = tuple(direction)\n",
        "            context = str(response) # response is the context for the next message\n",
        "        # remove the sentence pairs that does not have context but only responses\n",
        "        sentence_pairs = {k: v for k, v in sentence_pairs.items() if v[0] != ''}\n",
        "        dialogues[dialogue_id] = sentence_pairs\n",
        "\n",
        "    return dialogues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI75inovC8Ob"
      },
      "outputs": [],
      "source": [
        "text_df['text'] = text_df['text'].apply(preprocess_text)\n",
        "text_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiyYNcgTC8Oc"
      },
      "outputs": [],
      "source": [
        "dialogues = parse_dialogue(text_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-eD55p2C8Oc"
      },
      "outputs": [],
      "source": [
        "# convert nested dictionary to dataframe\n",
        "def dict_to_df(data):\n",
        "    rows = []\n",
        "    for dialogue_id, sentence_pairs in data.items():\n",
        "        for idx, pair in sentence_pairs.items():\n",
        "            rows.append([dialogue_id, idx, pair[0], pair[1]])\n",
        "    df = pd.DataFrame(rows, columns=['dialogueID', 'index', 'context', 'response'])\n",
        "    return df\n",
        "\n",
        "dialogue_df = dict_to_df(dialogues)\n",
        "dialogue_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Mp61AKC8Oc"
      },
      "outputs": [],
      "source": [
        "dialogue_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmdzwqwRC8Od"
      },
      "source": [
        "### Load the structured data into dataframe and index it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDvjGSneC8Od"
      },
      "source": [
        "- As the model contains a layer of embedding, lemmatization is not required. (words that are similar will have vectors that are close to each other)\n",
        "  - https://aclanthology.org/2021.nodalida-main.25/\n",
        "  - https://aclanthology.org/2021.nodalida-main.25.pdf\n",
        "- As one of the models will be using attention, removing stop words is not required. (attention will learn to ignore them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YOKINqrC8Od"
      },
      "outputs": [],
      "source": [
        "# use torch text to create vocabulary\n",
        "def tokenize(text, nlp=nlp):\n",
        "    \"\"\"\n",
        "    Tokenize text\n",
        "    :param text: text to be tokenized\n",
        "    :return: list of tokens\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in nlp.tokenizer(text)]\n",
        "\n",
        "def create_mapping(df, tokenize=tokenize):\n",
        "    \"\"\"\n",
        "    Create vocabulary mapping from context and response dataframes\n",
        "    :param df_context: context dataframe\n",
        "    :param df_response: response dataframe\n",
        "    :param tokenize: tokenization function\n",
        "    :return: vocabulary mapping\n",
        "    \"\"\"\n",
        "    # Create vocabulary mapping\n",
        "    vocab = set()\n",
        "    default_tokens = ['<pad>', '<bos>', '<eos>', '<unk>']\n",
        "    start_index = len(default_tokens)\n",
        "    for context, response in zip(df['context'], df['response']):\n",
        "        vocab.update(tokenize(context))\n",
        "        vocab.update(tokenize(response))\n",
        "    word2idx = {word: start_index+idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {start_index+idx: word for idx, word in enumerate(vocab)}\n",
        "    for idx, token in enumerate(default_tokens):\n",
        "        word2idx[token] = idx\n",
        "        idx2word[idx] = token\n",
        "    return word2idx, idx2word\n",
        "\n",
        "def lookup_words(idx2word, indices):\n",
        "    \"\"\"\n",
        "    Lookup words from indices\n",
        "    :param idx2word: index to word mapping\n",
        "    :param indices: indices to be converted\n",
        "    :return: list of words\n",
        "    \"\"\"\n",
        "    return [idx2word[idx] for idx in indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO_IhmeSC8Oe"
      },
      "outputs": [],
      "source": [
        "word2idx, idx2word = create_mapping(dialogue_df)\n",
        "word2idx['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCsbZWo0C8Oe"
      },
      "outputs": [],
      "source": [
        "# Map words to indices\n",
        "dialogue_df['context_idx'] = dialogue_df['context'].apply(lambda x: [word2idx[word] for word in tokenize(x)])\n",
        "dialogue_df['response_idx'] = dialogue_df['response'].apply(lambda x: [word2idx[word] for word in tokenize(x)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2e0T0drC8Oe"
      },
      "outputs": [],
      "source": [
        "# add bos and eos tokens to context and response\n",
        "bos = word2idx['<bos>']\n",
        "eos = word2idx['<eos>']\n",
        "dialogue_df['context_idx'] = dialogue_df['context_idx'].apply(lambda x: [bos] + x + [eos])\n",
        "dialogue_df['response_idx'] = dialogue_df['response_idx'].apply(lambda x: [bos] + x + [eos])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGH9L9rjC8Of"
      },
      "outputs": [],
      "source": [
        "dialogue_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGoSDAwJC8Of"
      },
      "outputs": [],
      "source": [
        "# save word2idx and idx2word\n",
        "files_to_save = ['vocab/word2idx.json', 'vocab/idx2word.json']\n",
        "if not os.path.exists('vocab'):\n",
        "    os.makedirs('vocab')\n",
        "\n",
        "for file in files_to_save:\n",
        "    if os.path.exists(file):\n",
        "        os.remove(file)\n",
        "    with open(file, 'w') as f:\n",
        "        if file == 'vocab/word2idx.json':\n",
        "            json.dump(word2idx, f)\n",
        "        elif file == 'vocab/idx2word.json':\n",
        "            json.dump(idx2word, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1NcRpnxC8Of"
      },
      "source": [
        "### Create tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvzU7ML5C8Of"
      },
      "outputs": [],
      "source": [
        "# Create tensors with sos, pad, eos tokens\n",
        "def create_tensors(df, max_len=20, min_len=1):\n",
        "    \"\"\"\n",
        "    Create tensors with sos, pad, eos tokens\n",
        "    :param df: dataframe with context and response\n",
        "    :param max_len: maximum length of sequence\n",
        "    :return: tensors with sos, pad, eos tokens\n",
        "    \"\"\"\n",
        "    # Create tensors\n",
        "    context_tensor = torch.zeros((len(df), max_len), dtype=torch.long)\n",
        "    response_tensor = torch.zeros((len(df), max_len), dtype=torch.long)\n",
        "    for i, (context, response) in enumerate(zip(df['context_idx'], df['response_idx'])):\n",
        "        # Trim context and response\n",
        "        if len(context) < max_len and len(context) >= min_len and len(response) < max_len and len(response) >= min_len:\n",
        "            context_tensor[i, :len(context)] = torch.tensor(context, dtype=torch.long)\n",
        "            response_tensor[i, :len(response)] = torch.tensor(response, dtype=torch.long)\n",
        "    # remove rows with all zeros\n",
        "    context_tensor = context_tensor[~(context_tensor == 0).all(1)]\n",
        "    response_tensor = response_tensor[~(response_tensor == 0).all(1)]\n",
        "\n",
        "    return context_tensor, response_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LxVAfa4C8Og"
      },
      "outputs": [],
      "source": [
        "# get max length of context and response\n",
        "max_len_context = max(dialogue_df['context_idx'].apply(len))\n",
        "max_len_response = max(dialogue_df['response_idx'].apply(len))\n",
        "max_len = max(max_len_context, max_len_response)\n",
        "max_len = 12 # override max_len to reduce training time\n",
        "min_len = 3\n",
        "print(f'Maximum length of sequence: {max_len}', f'Minimum length of sequence: {min_len}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-NDJo9rC8Og"
      },
      "source": [
        "- A filter is applied to remove sentences that are too long or too short.\n",
        "- Remove sentences that are too long is required as it will take a long time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIiNkT8HC8Og"
      },
      "outputs": [],
      "source": [
        "context_tensor, response_tensor = create_tensors(dialogue_df, max_len=max_len, min_len=min_len)\n",
        "print(context_tensor.shape, response_tensor.shape)\n",
        "print(f'Total data size: {context_tensor.shape[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGq2dxPC8Oh"
      },
      "source": [
        "### Split and batch the data\n",
        "\n",
        "Data is batched to reduce memory overhead and improve the speed of training. Data presented in the tensor is padded to be of equal lenght as demonstrated below:\n",
        "\n",
        "\n",
        "```\n",
        "[<bos>,can,you,help,me,with,a,support,issue,<eos>,<pad>,<pad>,<pad>,<pad>,<pad>]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZlE7v0lC8Oh"
      },
      "outputs": [],
      "source": [
        "class ContextReponseBatch:\n",
        "    def __init__(self, data):\n",
        "        transposed_data = list(zip(*data))\n",
        "        self.input = torch.stack(transposed_data[0], 0)\n",
        "        # self.input_mask = (self.input != 0)\n",
        "        self.target = torch.stack(transposed_data[1], 0)\n",
        "        # self.target_mask = (self.target != 0)\n",
        "\n",
        "    def pin_memory(self):\n",
        "        \"\"\"\n",
        "        Pin memory for faster data transfer to GPU\n",
        "        :return: self\n",
        "        \"\"\"\n",
        "        self.input = self.input.pin_memory()\n",
        "        # self.input_mask = self.input_mask.pin_memory()\n",
        "        self.target = self.target.pin_memory()\n",
        "        # self.target_mask = self.target_mask.pin_memory()\n",
        "        return self\n",
        "\n",
        "def collate_wrapper(batch):\n",
        "    \"\"\"\n",
        "    Wrapper for collate function\n",
        "    :param batch: batch of data\n",
        "    :return: ContextReponseBatch object\n",
        "    \"\"\"\n",
        "    return ContextReponseBatch(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJXNUwosC8Oh"
      },
      "outputs": [],
      "source": [
        "# Split data into train, validation, and test sets\n",
        "def split_data(context_tensor, response_tensor, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Split data into train, validation, and test sets\n",
        "    :param context_tensor: context tensor\n",
        "    :param response_tensor: response tensor\n",
        "    :param train_ratio: ratio of train set\n",
        "    :param val_ratio: ratio of validation set\n",
        "    :param test_ratio: ratio of test set\n",
        "    :return: train, validation, and test sets\n",
        "    \"\"\"\n",
        "    # Split data into train, validation, and test sets\n",
        "    dataset = TensorDataset(context_tensor, response_tensor)\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    val_size = int(val_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "# Batch data\n",
        "def batch_data(train_set, val_set, test_set, batch_size=8, fn=collate_wrapper):\n",
        "    \"\"\"\n",
        "    Batch data\n",
        "    :param train_set: train set\n",
        "    :param val_set: validation set\n",
        "    :param test_set: test set\n",
        "    :param batch_size: batch size\n",
        "    :return: train, validation, and test loaders\n",
        "    \"\"\"\n",
        "    # Batch data\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_wrapper)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, collate_fn=collate_wrapper)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, collate_fn=collate_wrapper)\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2DzdBMPC8Oh"
      },
      "outputs": [],
      "source": [
        "# Create train, validation, and test sets\n",
        "train_set, val_set, test_set = split_data(context_tensor, response_tensor)\n",
        "\n",
        "# Batch data\n",
        "train_loader, val_loader, test_loader = batch_data(train_set, val_set, test_set, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVzbffmpC8Oh"
      },
      "outputs": [],
      "source": [
        "# preview shape of batch\n",
        "next(iter(train_loader)).input.shape # (batch_size, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG0GSwzyC8Oi"
      },
      "source": [
        "# Model Development\n",
        "\n",
        "The seq-2-seq model is based on a reference implementation provided by pytorch for language transaltion https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html. It include the following key components:\n",
        "- Gated recurrent unit (GRU) RNN encoder\n",
        "- GRU RNN decoder\n",
        "- Luong attention\n",
        "\n",
        "Gradient clipping is used to avoid exploding gradients, and dropout layers are used to avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4IopPb-C8Oi"
      },
      "source": [
        "## Building the seq2seq model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQJjxbfMC8Oi"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize weights\n",
        "    :param m: model\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.xavier_uniform_(param.data)\n",
        "        elif 'bias' in name:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOWTDhRZC8Oi"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    GRU RNN Encoder\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float = 0):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # dimension of imput\n",
        "        self.input_dim = input_dim\n",
        "        # dimension of embedding layer\n",
        "        self.emb_dim = emb_dim\n",
        "        # dimension of encoding hidden layer\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        # dimension of decoding hidden layer\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "\n",
        "        # create embedding layer use to train embedding representations of the corpus\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "        # use GRU for RNN\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True, batch_first=False, num_layers=1)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        # create dropout layer which will help produce a more generalisable model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # apply dropout to the embedding layer\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # generate an output and hidden layer from the rnn\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Luong attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attn_dim: int):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        # dimension of encoding hidden layer\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        # dimension of decoding hidden layer\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n",
        "\n",
        "        self.attn = nn.Linear(self.attn_in, attn_dim)\n",
        "\n",
        "    def forward(self,\n",
        "                decoder_hidden: torch.Tensor,\n",
        "                encoder_outputs: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # Luong attention\n",
        "        energy = torch.tanh(self.attn(torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)))\n",
        "        attention = torch.sum(energy, dim=2)\n",
        "\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    GRU RNN Decoder with attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 attention: nn.Module,\n",
        "                 dropout: float = 0):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "\n",
        "        # dimention of output layer\n",
        "        self.output_dim = output_dim\n",
        "        # dimention of embedding layer\n",
        "        self.emb_dim = emb_dim\n",
        "        # dimention of encoding hidden layer\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        # dimention of decoding hidden layer\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        # drouput rate\n",
        "        self.dropout = dropout\n",
        "        # attention layer\n",
        "        self.attention = attention\n",
        "\n",
        "        # create embedding layer use to train embedding representations of the corpus\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        # use GRU for RNN\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=False, num_layers=1)\n",
        "        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def encode_attention(self,\n",
        "                              decoder_hidden: torch.Tensor,\n",
        "                              encoder_outputs: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        a = self.attention(decoder_hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n",
        "        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n",
        "        return weighted_encoder_rep\n",
        "\n",
        "    def forward(self,\n",
        "                input: torch.Tensor,\n",
        "                decoder_hidden: torch.Tensor,\n",
        "                encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # apply dropout to embedding layer\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        weighted_encoder = self.encode_attention(decoder_hidden, encoder_outputs)\n",
        "\n",
        "        # generate an output and hidden layer from the rnn\n",
        "        rnn_input = torch.cat((embedded, weighted_encoder), dim=2)\n",
        "        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n",
        "\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted_encoder = weighted_encoder.squeeze(0)\n",
        "        output = self.out(torch.cat((output, weighted_encoder, embedded), dim=1))\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    GRU RNN Decoder without attention\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 output_dim: int,\n",
        "                 emb_dim: int,\n",
        "                 enc_hid_dim: int,\n",
        "                 dec_hid_dim: int,\n",
        "                 dropout: float = 0):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # dimention of output layer\n",
        "        self.output_dim = output_dim\n",
        "        # dimention of embedding layer\n",
        "        self.emb_dim = emb_dim\n",
        "        # dimention of encoding hidden layer\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        # dimention of decoding hidden layer\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        # drouput rate\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # create embedding layer use to train embedding representations of the corpus\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        # GRU RNN\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=False, num_layers=1)\n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                input: torch.Tensor,\n",
        "                decoder_hidden: torch.Tensor,\n",
        "                encoder_outputs: torch.Tensor) -> Tuple[torch.Tensor\n",
        "                                                        , torch.Tensor]:\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # apply dropout to embedding layer\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        context = encoder_outputs[-1,:,:]\n",
        "        context = context.repeat(embedded.shape[0], 1, 1)\n",
        "        embs_and_context = torch.cat((embedded, context), -1)\n",
        "        # generate an output and hidden layer from the rnn\n",
        "        output, decoder_hidden = self.rnn(embs_and_context, decoder_hidden.unsqueeze(0))\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        context = context.squeeze(0)\n",
        "        output = self.out(torch.cat((output, embedded, context), -1))\n",
        "        return output, decoder_hidden.squeeze(0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Seq-2-Seq model combining RNN encoder and RNN decoder\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 decoder: nn.Module,\n",
        "                 device: torch.device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self,\n",
        "                src: torch.Tensor,\n",
        "                trg: torch.Tensor,\n",
        "                teacher_forcing_ratio: float = 0.5) -> torch.Tensor:\n",
        "        src = src.transpose(0, 1) # (max_len, batch_size)\n",
        "        trg = trg.transpose(0, 1) # (max_len, batch_size)\n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # first input to the decoder is the <sos> token\n",
        "        output = trg[0,:]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output, hidden, encoder_outputs)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUGC4lxvC8Oj"
      },
      "outputs": [],
      "source": [
        "test_batch = next(iter(train_loader))\n",
        "test_batch.input.shape, test_batch.target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtplHJLTC8Oj"
      },
      "outputs": [],
      "source": [
        "# enc = Encoder(input_dim=len(word2idx), emb_dim=256, enc_hid_dim=512, dec_hid_dim=512)\n",
        "# attn = Attention(enc_hid_dim=512, dec_hid_dim=512, attn_dim=64)\n",
        "# dec = AttnDecoder(output_dim=len(word2idx), emb_dim=256, enc_hid_dim=512, dec_hid_dim=512, attention=attn)\n",
        "# model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "# model.apply(init_weights)\n",
        "# model.to(device)\n",
        "# model.train()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# optimizer.zero_grad()\n",
        "# model(test_batch.input.to(device), test_batch.target.to(device), teacher_forcing_ratio=0.5).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M66QKHs5C8Ok"
      },
      "outputs": [],
      "source": [
        "print(len(word2idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TamNlRoOC8Ok"
      },
      "outputs": [],
      "source": [
        "# enc = Encoder(input_dim=len(word2idx), emb_dim=256, enc_hid_dim=512, dec_hid_dim=512)\n",
        "# dec = Decoder(output_dim=len(word2idx), emb_dim=256, enc_hid_dim=512, dec_hid_dim=512)\n",
        "# model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "# model.apply(init_weights)\n",
        "# model.to(device)\n",
        "# model.train()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# optimizer.zero_grad()\n",
        "# model(test_batch.input.to(device), test_batch.target.to(device), teacher_forcing_ratio=0.5).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbh5vSnjC8Ok"
      },
      "source": [
        "## Training the seq2seq model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mq5W-wnC8Ok"
      },
      "outputs": [],
      "source": [
        "PAD_INDEX = word2idx['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_INDEX)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer,\n",
        "                run_name = 'seq2seq', init_weights=init_weights, device=device,\n",
        "                n_epochs=10, clip=1, criterion=criterion,\n",
        "                teacher_forcing_ratio=0.5, params=None):\n",
        "    \"\"\"\n",
        "    Train model\n",
        "    :param model: model\n",
        "    :param train_loader: train loader\n",
        "    :param val_loader: validation loader\n",
        "    :param optimizer: optimizer\n",
        "    :param n_epochs: number of epochs\n",
        "    :param clip: clip\n",
        "    :param criterion: loss function\n",
        "    :param PAD_INDEX: index for pad token\n",
        "    :param teacher_forcing_ratio: teacher forcing ratio\n",
        "    :return: model, train loss, validation loss\n",
        "    \"\"\"\n",
        "    if not os.path.exists('models'):\n",
        "        os.makedirs('models')\n",
        "    model = model.to(device)\n",
        "    model.apply(init_weights)\n",
        "    hyperparams = {'n_epochs': n_epochs,\n",
        "                    'clip': clip,\n",
        "                    'teacher_forcing_ratio': teacher_forcing_ratio}\n",
        "    if params:\n",
        "        hyperparams.update(params)\n",
        "    writer = SummaryWriter(f'runs/{run_name}')\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        epoch_loss = 1\n",
        "        val_epoch_loss = 1\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            src = batch.input.to(device)\n",
        "            trg = batch.target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg.transpose(0, 1)\n",
        "            trg = trg[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            loss.to(device).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip) # clip gradients\n",
        "            optimizer.step() # update parameters\n",
        "            epoch_loss += loss.item() # update epoch loss\n",
        "            writer.add_scalar('Train Loss', loss.item(), epoch * len(train_loader) + i)\n",
        "        train_loss.append(epoch_loss / len(train_loader))\n",
        "        # save model with datetime and epoch\n",
        "        torch.save(model.state_dict(), f'models/{run_name}_epoch{epoch+1}.pt')\n",
        "        # remove previous model\n",
        "        if epoch > 0:\n",
        "            os.remove(f'models/{run_name}_epoch{epoch}.pt')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(val_loader):\n",
        "                src = batch.input.to(device)\n",
        "                trg = batch.target.to(device)\n",
        "\n",
        "                output = model(src, trg, teacher_forcing_ratio)\n",
        "                output_dim = output.shape[-1]\n",
        "                output = output[1:].view(-1, output_dim)\n",
        "                trg = trg.transpose(0, 1)\n",
        "                trg = trg[1:].reshape(-1)\n",
        "\n",
        "                loss = criterion(output, trg)\n",
        "                val_epoch_loss += loss.item()\n",
        "        val_loss.append(val_epoch_loss / len(val_loader))\n",
        "        writer.add_scalar('Validation Loss', val_epoch_loss / len(val_loader), epoch)\n",
        "        duration = time.time()-start_time\n",
        "        remaining = duration * (n_epochs - epoch - 1)\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {duration:.3f}s | Train Loss: {epoch_loss/len(train_loader):.3f} | Val Loss: {val_epoch_loss/len(val_loader):.3f} | Remaining: {remaining//60:.0f}m {remaining%60:.0f}s')\n",
        "    writer.add_hparams(hyperparams, {'hparam/train_loss': train_loss[-1], 'hparam/val_loss': val_loss[-1]})\n",
        "    writer.close()\n",
        "    return model, train_loss, val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEqSdJ-AC8Oq"
      },
      "outputs": [],
      "source": [
        "params = {'input_dim': len(word2idx),\n",
        "            'emb_dim': 192,\n",
        "            'enc_hid_dim': 256,\n",
        "            'dec_hid_dim': 256,\n",
        "            'dropout': 0.5,\n",
        "            'attn_dim': 64,\n",
        "            'teacher_forcing_ratio': 0.5,\n",
        "            'epochs': 35}\n",
        "print(device)\n",
        "batch_size = 36"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7TgHoPsC8Oq"
      },
      "outputs": [],
      "source": [
        "# Create train, validation, and test sets\n",
        "train_set, val_set, test_set = split_data(context_tensor, response_tensor)\n",
        "\n",
        "# Batch data\n",
        "train_loader, val_loader, test_loader = batch_data(train_set, val_set, test_set, batch_size=batch_size)\n",
        "\n",
        "print(f'Training set size: {len(train_set)}', f'Validation set size: {len(val_set)}', f'Test set size: {len(test_set)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaWy5yyrC8Or"
      },
      "outputs": [],
      "source": [
        "# enc = Encoder(input_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], dropout=params['dropout'])\n",
        "# dec = Decoder(output_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], dropout=params['dropout'])\n",
        "# model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f'Model has {num_params:,} trainable parameters')\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# norm_model, train_loss, val_loss = train_model(model, train_loader, val_loader, optimizer,\n",
        "#                                                run_name=f'NormSeq2Seq-{int(num_params/1e6)}M', n_epochs=params['epochs'],\n",
        "#                                                teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "#                                                params=params)\n",
        "\n",
        "params = {\n",
        "    'input_dim': params['input_dim'],\n",
        "    'emb_dim': max(64, params['emb_dim'] // 2),\n",
        "    'enc_hid_dim': max(128, params['enc_hid_dim'] // 2),\n",
        "    'dec_hid_dim': max(128, params['dec_hid_dim'] // 2),\n",
        "    'dropout': 0.3,\n",
        "    'epochs': 25,\n",
        "    'teacher_forcing_ratio': 0.7\n",
        "}\n",
        "\n",
        "enc = Encoder(\n",
        "    input_dim=params['input_dim'],\n",
        "    emb_dim=params['emb_dim'],\n",
        "    enc_hid_dim=params['enc_hid_dim'],\n",
        "    dec_hid_dim=params['dec_hid_dim'],\n",
        "    dropout=params['dropout']\n",
        ")\n",
        "\n",
        "dec = Decoder(\n",
        "    output_dim=params['input_dim'],\n",
        "    emb_dim=params['emb_dim'],\n",
        "    enc_hid_dim=params['enc_hid_dim'],\n",
        "    dec_hid_dim=params['dec_hid_dim'],\n",
        "    dropout=params['dropout']\n",
        ")\n",
        "\n",
        "model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model has {num_params:,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-5)\n",
        "\n",
        "norm_model, train_loss, val_loss = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    run_name=f'NormSeq2Seq-{int(num_params/1e6)}M',\n",
        "    n_epochs=params['epochs'],\n",
        "    teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "    params=params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phpxgaKWC8Os"
      },
      "outputs": [],
      "source": [
        "# enc = Encoder(input_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], dropout=params['dropout'])\n",
        "# attn = Attention(enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], attn_dim=params['attn_dim'])\n",
        "# dec = AttnDecoder(output_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], attention=attn, dropout=params['dropout'])\n",
        "# model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f'Model has {num_params:,} trainable parameters')\n",
        "# optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# attn_model, train_loss, val_loss = train_model(model, train_loader, val_loader, optimizer,\n",
        "#                                                run_name=f'AttnSeq2Seq-{int(num_params/1e6)}M', n_epochs=params['epochs'],\n",
        "#                                                teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "#                                                params=params)\n",
        "\n",
        "params = {\n",
        "    'input_dim': params['input_dim'],\n",
        "    'emb_dim': max(64, params['emb_dim'] // 2),\n",
        "    'enc_hid_dim': max(128, params['enc_hid_dim'] // 2),\n",
        "    'dec_hid_dim': max(128, params['dec_hid_dim'] // 2),\n",
        "    'attn_dim': max(64, params['attn_dim'] // 2),\n",
        "    'dropout': 0.3,\n",
        "    'epochs': 25,\n",
        "    'teacher_forcing_ratio': 0.7\n",
        "}\n",
        "\n",
        "enc = Encoder(\n",
        "    input_dim=params['input_dim'],\n",
        "    emb_dim=params['emb_dim'],\n",
        "    enc_hid_dim=params['enc_hid_dim'],\n",
        "    dec_hid_dim=params['dec_hid_dim'],\n",
        "    dropout=params['dropout']\n",
        ")\n",
        "\n",
        "attn = Attention(\n",
        "    enc_hid_dim=params['enc_hid_dim'],\n",
        "    dec_hid_dim=params['dec_hid_dim'],\n",
        "    attn_dim=params['attn_dim']\n",
        ")\n",
        "\n",
        "dec = AttnDecoder(\n",
        "    output_dim=params['input_dim'],\n",
        "    emb_dim=params['emb_dim'],\n",
        "    enc_hid_dim=params['enc_hid_dim'],\n",
        "    dec_hid_dim=params['dec_hid_dim'],\n",
        "    attention=attn,\n",
        "    dropout=params['dropout']\n",
        ")\n",
        "\n",
        "model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model has {num_params:,} trainable parameters')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=1e-3,\n",
        "                       weight_decay=1e-5)\n",
        "\n",
        "attn_model, train_loss, val_loss = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    run_name=f'AttnSeq2Seq-{int(num_params/1e6)}M',\n",
        "    n_epochs=params['epochs'],\n",
        "    teacher_forcing_ratio=params['teacher_forcing_ratio'],\n",
        "    params=params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAhzVa_mC8Os"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "The model evaluated against the following metrics:\n",
        "- Bleu score: measures the similarity of the input text to the validation set\n",
        "- Cosine similarity: measures the cosine similarity of the embedding representation of text to the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkJsnN5pC8Os"
      },
      "source": [
        "## Using the seq2seq models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HCMLldcC8Os"
      },
      "outputs": [],
      "source": [
        "def generate(model, sentence, max_len=30, word2idx=word2idx, idx2word=idx2word, device=device, tokenize=tokenize, preprocess_text=preprocess_text, lookup_words=lookup_words):\n",
        "    \"\"\"\n",
        "    Generate response\n",
        "    :param model: model\n",
        "    :param sentence: sentence\n",
        "    :param max_len: maximum length of sequence\n",
        "    :param word2idx: word to index mapping\n",
        "    :param idx2word: index to word mapping\n",
        "    :return: response\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sentence = preprocess_text(sentence)\n",
        "    tokens = tokenize(sentence)\n",
        "    tokens = [word2idx[token] if token in word2idx else word2idx['<unk>'] for token in tokens]\n",
        "    tokens = [word2idx['<bos>']] + tokens + [word2idx['<eos>']]\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device)\n",
        "    outputs = [word2idx['<bos>']]\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(tokens)\n",
        "    for t in range(max_len):\n",
        "        output, hidden = model.decoder(torch.tensor([outputs[-1]], dtype=torch.long).to(device), hidden, encoder_outputs)\n",
        "        top1 = output.max(1)[1]\n",
        "        outputs.append(top1.item())\n",
        "        if top1.item() == word2idx['<eos>']:\n",
        "            break\n",
        "    response = lookup_words(idx2word, outputs)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L26ZupgqC8Ot"
      },
      "outputs": [],
      "source": [
        "# enc = Encoder(input_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], dropout=params['dropout'])\n",
        "# dec = Decoder(output_dim=params['input_dim'], emb_dim=params['emb_dim'], enc_hid_dim=params['enc_hid_dim'], dec_hid_dim=params['dec_hid_dim'], dropout=params['dropout'])\n",
        "# norm_model = Seq2Seq(encoder=enc, decoder=dec, device=device)\n",
        "# norm_model.load_state_dict(torch.load('models/NormSeq2Seq-188M_epoch35.pt'))\n",
        "# norm_model.to(device)\n",
        "# print(f'Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylRu__yKC8Ot"
      },
      "outputs": [],
      "source": [
        "test = 'is ubuntu good?'\n",
        "norm_test = generate(norm_model, test, max_len=12, word2idx=word2idx, idx2word=idx2word, device=device)\n",
        "' '.join(norm_test).replace('<bos>', '').replace('<eos>', '').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX8PYnqHC8Ot"
      },
      "outputs": [],
      "source": [
        "test = 'is ubuntu good?'\n",
        "attn_test = generate(attn_model, test, max_len=12, word2idx=word2idx, idx2word=idx2word, device=device)\n",
        "' '.join(attn_test).replace('<bos>', '').replace('<eos>', '').strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnyDkBHRC8Ou"
      },
      "source": [
        "## Evaluating the seq2seq models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys7_hb8sC8Ou"
      },
      "outputs": [],
      "source": [
        "def tensor_to_text(tensor, idx2word=idx2word):\n",
        "    \"\"\"\n",
        "    Convert tensor to text\n",
        "    :param tensor: tensor\n",
        "    :param idx2word: index to word mapping\n",
        "    :return: text\n",
        "    \"\"\"\n",
        "    textlist = tensor.tolist()\n",
        "    text = lookup_words(idx2word, textlist)\n",
        "    # remove default tokens\n",
        "    text = [word for word in text if word not in ['<bos>', '<eos>', '<pad>', '<unk>']]\n",
        "    return text\n",
        "\n",
        "def predict_from_tensor(model, input_tensor, max_len=12, word2idx=word2idx, idx2word=idx2word, device=device, lookup_words=lookup_words):\n",
        "    \"\"\"\n",
        "    Predict response\n",
        "    :param model: model\n",
        "    :param input_tensor: input tensor\n",
        "    :param max_len: maximum length of sequence\n",
        "    :param word2idx: word to index mapping\n",
        "    :param idx2word: index to word mapping\n",
        "    :return: response\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    input_tensor = input_tensor.unsqueeze(1).to(device)\n",
        "    outputs = [word2idx['<bos>']]\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(input_tensor)\n",
        "    for t in range(max_len):\n",
        "        output, hidden = model.decoder(torch.tensor([outputs[-1]], dtype=torch.long).to(device), hidden, encoder_outputs)\n",
        "        top1 = output.max(1)[1]\n",
        "        outputs.append(top1.item())\n",
        "        if top1.item() == word2idx['<eos>']:\n",
        "            break\n",
        "    response = lookup_words(idx2word, outputs)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4uEMajZC8Ou"
      },
      "source": [
        "### BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmhiwhSCC8Ou"
      },
      "outputs": [],
      "source": [
        "def eval_pair(model, data, max_len=12,\n",
        "             word2idx=word2idx,\n",
        "             idx2word=idx2word, device=device,\n",
        "             tensor_to_text=tensor_to_text,\n",
        "             predict_from_tensor=predict_from_tensor):\n",
        "    predicted = []\n",
        "    actual = []\n",
        "    for c, r in data.dataset:\n",
        "        predict = predict_from_tensor(model, c, max_len=max_len, word2idx=word2idx, idx2word=idx2word, device=device)\n",
        "        predict = [word for word in predict if word not in ['<bos>', '<eos>', '<pad>', '<unk>']]\n",
        "        predicted.append([predict])\n",
        "        actual.append(tensor_to_text(r))\n",
        "    return predicted, actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYS71K8xC8Ou"
      },
      "outputs": [],
      "source": [
        "attn_predicted, attn_actual = eval_pair(attn_model, test_loader, max_len=12)\n",
        "norm_predicted, norm_actual = eval_pair(norm_model, test_loader, max_len=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UC8fQd4C8Ou"
      },
      "outputs": [],
      "source": [
        "attn_bleu = corpus_bleu(attn_predicted, attn_actual)\n",
        "print(f'BLEU score for Seq2Seq with Attention: {attn_bleu*100:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSkr1StgC8Ov"
      },
      "outputs": [],
      "source": [
        "norm_bleu = corpus_bleu(norm_predicted, norm_actual)\n",
        "print(f'BLEU score for Seq2Seq without Attention: {norm_bleu*100:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60EuEzHwC8Ov"
      },
      "source": [
        "### Cosine Similarity\n",
        "- Cosine similarity is used to measure the similarity between two sentences.\n",
        "- First, sentence embeddings are created by encoding using a pre-trained model (BERT).\n",
        "- Next, the cosine similarity is calculated between the sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1-Hl_iiC8Ov"
      },
      "outputs": [],
      "source": [
        "def list_to_sent(list_of_words):\n",
        "    \"\"\"\n",
        "    Convert list of words to sentence\n",
        "    :param list_of_words: list of words\n",
        "    :return: sentence\n",
        "    \"\"\"\n",
        "    return ' '.join(list_of_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkoymBWsC8Ov"
      },
      "outputs": [],
      "source": [
        "sbert = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HqyWq-fC8Ov"
      },
      "outputs": [],
      "source": [
        "def get_cosines(list_pred, act, sbert=sbert, list_to_sent=list_to_sent):\n",
        "    \"\"\"\n",
        "    Get cosine similarity scores\n",
        "    :param list_pred: list of list of predicted sentences [[a],[b]]\n",
        "    :param act: list of actual sentences [a,b]\n",
        "    :param sbert: sentence transformer model\n",
        "    :param list_to_sent: function to convert list of words to sentence\n",
        "    :return: cosine similarity scores\n",
        "    \"\"\"\n",
        "    sent_pred = [val for sublist in list_pred for val in sublist]\n",
        "    sent_pred = [list_to_sent(sent) for sent in sent_pred]\n",
        "    sent_act = [list_to_sent(sent) for sent in act]\n",
        "    cosine_scores = []\n",
        "    for pred, act in zip(sent_pred, sent_act):\n",
        "        pred = sbert.encode(pred)\n",
        "        act = sbert.encode(act)\n",
        "        cosine_scores.append(cosine(pred, act))\n",
        "    cosine_scores = np.array(cosine_scores)\n",
        "    return cosine_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_g4XrfVC8Ov"
      },
      "outputs": [],
      "source": [
        "attn_cosine = get_cosines(attn_predicted, attn_actual)\n",
        "norm_cosine = get_cosines(norm_predicted, norm_actual)\n",
        "\n",
        "print(f'Average cosine similarity for Seq2Seq with Attention: {np.mean(attn_cosine):.2f}')\n",
        "print(f'Average cosine similarity for Seq2Seq without Attention: {np.mean(norm_cosine):.2f}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}